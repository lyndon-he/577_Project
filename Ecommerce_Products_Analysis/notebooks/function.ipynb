{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime, nltk, warnings\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors, linear_model, svm, tree, ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function analyzes the content of the **Description** column by performing the following operations:\n",
    "\n",
    "- extract names appearing in the products description\n",
    "- for each name, extract the root of the word and aggregate the set of names associated with this particular root\n",
    "- count the number of times each root appears in the dataframe\n",
    "- when several words are listed in the same root, choose the shortest name.This systematically selects the singular when there are singular/plural variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "def keywords_inventory(dataframe, column = 'Description'):\n",
    "    \"\"\"Extract features from text\"\"\"\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "    keywords_roots  = dict()  # collect the words / root\n",
    "    keywords_select = dict()  # association: root <-> keyword\n",
    "    category_keys   = []\n",
    "    count_keywords  = dict()\n",
    "    icount = 0\n",
    "    for s in dataframe[column]:\n",
    "        if pd.isnull(s): continue\n",
    "        lines = s.lower()\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "        \n",
    "        for t in nouns:\n",
    "            t = t.lower() ; stm = stemmer.stem(t)\n",
    "            if stm in keywords_roots:                \n",
    "                keywords_roots[stm].add(t)\n",
    "                count_keywords[stm] += 1                \n",
    "            else:\n",
    "                keywords_roots[stm] = {t}\n",
    "                count_keywords[stm] = 1\n",
    "    \n",
    "    for s in keywords_roots.keys():\n",
    "        if len(keywords_roots[s]) > 1:  \n",
    "            min_length = 1000\n",
    "            for k in keywords_roots[s]:\n",
    "                if len(k) < min_length:\n",
    "                    clef = k ; min_length = len(k)            \n",
    "            category_keys.append(clef)\n",
    "            keywords_select[s] = clef\n",
    "        else:\n",
    "            category_keys.append(list(keywords_roots[s])[0])\n",
    "            keywords_select[s] = list(keywords_roots[s])[0]\n",
    "                   \n",
    "    print(\"Number of keywords in variable '{}': {}\".format(column,len(category_keys)))\n",
    "    return category_keys, keywords_roots, keywords_select, count_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n",
    "    \"\"\"Create the silhouette value plot for each group\"\"\"\n",
    "    plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
    "\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(8, 8)\n",
    "    ax1.set_xlim([lim_x[0], lim_x[1]])\n",
    "    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "      \n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / n_clusters)        \n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
    "                           facecolor=color, edgecolor=color, alpha=0.8)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n",
    "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_color_func(word=None, font_size=None, position=None,\n",
    "                      orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * tone / 255.0)\n",
    "    s = int(100.0 * 255.0 / 255.0)\n",
    "    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "\n",
    "\n",
    "def make_wordcloud(liste, increment):\n",
    "    \"\"\"Plot word cloud of each group\"\"\"\n",
    "    ax1 = fig.add_subplot(5,2,increment)\n",
    "    words = dict()\n",
    "    trunc_occurences = liste[0:150]\n",
    "    for s in trunc_occurences:\n",
    "        words[s[0]] = s[1]\n",
    "\n",
    "    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n",
    "                          max_words=1628,relative_scaling=1,\n",
    "                          color_func = random_color_func,\n",
    "                          normalize_plurals=False)\n",
    "    wordcloud.generate_from_frequencies(words)\n",
    "    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    ax1.axis('off')\n",
    "    plt.title('cluster nÂº{}'.format(increment-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
